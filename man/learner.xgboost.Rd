% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/learner.xgboost.R
\name{learner.xgboost}
\alias{learner.xgboost}
\title{Basic xgboost learner}
\usage{
learner.xgboost(data = data_train_numeric_clean_imputed, lasso = FALSE,
  boruta = FALSE)
}
\arguments{
\item{data}{Input data which is default set to the numeric, imputed and cleaned training dataset}

\item{lasso}{Boolean flag that shrinks data to the features of \code{feature.lasso()} stored in
the variable \code{features_lasso}}

\item{boruta}{Boolean flag that shrinks data to the features of \code{boruta.lasso()} stored in
the variable \code{features_boruta}}
}
\value{
0 as error output if both flags are set to true
}
\description{
Parallel tuning function using xgboost that is either based on glmnet, boruta or the whole dataset.
}
\details{
The default execution uses the whole training dataset. By setting either the \code{lasso} or
\code{boruta} parameter to true the number of features is reduced according to the results of
the feature selection. The number of rounds is set to 500 because including that parameter into
the set of tuning parameters leads to worse results. The xgboost learner is wrapper in a Filter
wrapper that uses chi squared as feature selection method. The result is three times cross validated
at maximum 1000 experiments using irace as a control structure.
}
\examples{
KaggleHouse:::learner.xgboost(lasso=TRUE)

}

